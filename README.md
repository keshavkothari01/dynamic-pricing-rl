# ğŸ¯ Dynamic Pricing using Reinforcement Learning

**A complete academic project demonstrating AI-powered dynamic pricing optimization**

---

## ğŸ“‹ Project Overview

### Title
**Dynamic Pricing using Reinforcement Learning**

### Objective
To build a Reinforcement Learning (RL) based agent that learns to set optimal prices dynamically for a product or service, in order to maximize total revenue or profit. The agent interacts with a simulated market environment built using synthetic data, where customer demand depends on factors like price, competitor pricing, and seasonality.

### Problem Statement
Traditional businesses use static or rule-based pricing models. However, in real-world scenarios like e-commerce, airline ticketing, and ride-hailing, demand constantly changes, and so should prices.

This project aims to simulate such an environment and train an AI agent that:
- Observes the market (demand trends, competitor price)
- Chooses prices dynamically
- Learns pricing strategy through reward feedback (profit)
- Outperforms fixed or random pricing methods

---

## ğŸ—ï¸ Project Structure

```
dynamic_pricing_rl_project/
â”‚
â”œâ”€â”€ ğŸ“ notebooks/
â”‚   â”œâ”€â”€ 1_data_generation.ipynb         # Simulate demand dataset
â”‚   â”œâ”€â”€ 2_environment_creation.ipynb    # Define & test Gym environment
â”‚   â”œâ”€â”€ 3_agent_training.ipynb          # Train RL agent (PPO)
â”‚   â”œâ”€â”€ 4_evaluation_analysis.ipynb     # Evaluate, visualize, compare baselines
â”‚   â””â”€â”€ 5_report_summary.ipynb          # Final analysis & presentation
â”‚
â”œâ”€â”€ ğŸ“ saved_models/
â”‚   â””â”€â”€ pricing_agent_ppo.zip           # Trained PPO model
â”‚
â”œâ”€â”€ ğŸ“ data/
â”‚   â””â”€â”€ simulated_demand.csv            # Synthetic demand data
â”‚
â”œâ”€â”€ ğŸ“ visuals/
â”‚   â”œâ”€â”€ price_vs_demand.png             # Price-demand relationship
â”‚   â”œâ”€â”€ reward_curve.png                # Learning progress
â”‚   â””â”€â”€ performance_comparison.png      # Strategy comparison
â”‚
â”œâ”€â”€ streamlit_app.py                    # ğŸŒŸ Interactive dashboard
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ SETUP_GUIDE.md                      # Installation instructions
â”œâ”€â”€ STREAMLIT_GUIDE.md                  # Dashboard usage guide
â”œâ”€â”€ PROJECT_COMPLETE.md                 # Success summary
â””â”€â”€ STEP_BY_STEP_GUIDE.md               # ğŸ“š Detailed file-by-file guide
```

---

## ğŸ› ï¸ Technical Stack

| Component | Technology |
|-----------|-----------|
| **Programming Language** | Python 3.x |
| **RL Framework** | Stable-Baselines3 |
| **Environment** | Gymnasium (OpenAI Gym) |
| **Algorithm** | PPO (Proximal Policy Optimization) |
| **Data Processing** | NumPy, Pandas |
| **Visualization** | Matplotlib, Seaborn |
| **Interactive Dashboard** | Streamlit |
| **Deep Learning Backend** | PyTorch |

---

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8 or higher
- pip package manager

### Setup Instructions

1. **Clone or download the project**
   ```bash
   cd dynamic_pricing_rl_project
   ```

2. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

3. **Verify installation**
   ```bash
   python -c "import gymnasium, stable_baselines3; print('âœ… Installation successful!')"
   ```

---

## ğŸš€ Usage

### Option 1: Interactive Streamlit Dashboard ğŸŒŸ (Recommended!)

Run the real-time interactive visualization:

```bash
streamlit run streamlit_app.py
```

**Features:**
- ğŸ® Real-time simulation controls
- ğŸ“Š Live charts comparing RL vs baselines
- ğŸ’° Revenue tracking and performance metrics
- ğŸ† Strategy comparison and winner announcement
- âš™ï¸ Configurable simulation parameters

See [STREAMLIT_GUIDE.md](STREAMLIT_GUIDE.md) for detailed instructions.

### Option 2: Jupyter Notebooks

Execute notebooks in order:

1. **Data Generation** (`1_data_generation.ipynb`)
   - Generates synthetic demand data
   - Simulates price elasticity, seasonality, and competitor pricing
   - Saves data to `data/simulated_demand.csv`

2. **Environment Creation** (`2_environment_creation.ipynb`)
   - Defines custom `DynamicPricingEnv` Gym environment
   - Sets up observation and action spaces
   - Tests environment with random actions

3. **Agent Training** (`3_agent_training.ipynb`)
   - Initializes PPO agent
   - Trains for 100,000 timesteps
   - Saves trained model to `saved_models/`
   - Visualizes learning curve

4. **Evaluation & Analysis** (`4_evaluation_analysis.ipynb`)
   - Loads trained model
   - Compares RL agent vs. baselines (Fixed, Random)
   - Generates performance visualizations
   - Analyzes pricing behavior

5. **Report Summary** (`5_report_summary.ipynb`)
   - Complete project summary
   - Key findings and conclusions
   - Future work recommendations

---

## ğŸ“Š Key Results

### Performance Comparison

The RL agent (PPO) demonstrates superior performance:

- **âœ… Revenue Maximization**: Highest total revenue among all strategies
- **âœ… Adaptive Pricing**: Dynamically adjusts to market conditions
- **âœ… Learning Efficiency**: Converges within 100K timesteps
- **âœ… Robustness**: Maintains performance across seasons

### Baseline Comparisons

| Strategy | Description | Performance |
|----------|-------------|-------------|
| **RL Agent (PPO)** | Learned dynamic pricing | ğŸ† Best |
| **Fixed (â‚¹100)** | Constant price at â‚¹100 | Baseline |
| **Fixed (â‚¹120)** | Constant price at â‚¹120 | Below optimal |
| **Random** | Random price selection | Worst |

---

## ğŸ¯ Real-World Applications

This approach can be applied to:

1. **E-commerce**: Dynamic product pricing based on demand
2. **Airlines**: Ticket pricing optimization
3. **Ride-hailing**: Surge pricing (Uber, Lyft)
4. **Hotels**: Room rate optimization
5. **Cloud Services**: Resource pricing
6. **Energy**: Electricity pricing (peak/off-peak)

---

## ğŸ§  Technical Details

### Environment Specifications

- **Observation Space**: `[day_normalized, last_price_normalized, competitor_price_normalized]`
- **Action Space**: Discrete(11) - prices from â‚¹50 to â‚¹150 (step: â‚¹10)
- **Reward Function**: `Revenue = Price Ã— Demand` (normalized)
- **Episode Length**: 365 days (1 year)

### Demand Function

```python
demand = base_demand Ã— price_elasticity Ã— seasonal_factor Ã— competitor_effect Ã— noise
```

Where:
- **Price Elasticity**: -1.5 (demand decreases with price)
- **Seasonal Factor**: Sine wave (peaks during festive seasons)
- **Competitor Effect**: Reduces demand if competitor price is lower
- **Noise**: Random market uncertainty (Â±10%)

---

## ğŸ“ˆ Future Improvements

1. **Real Data Integration**: Use actual sales and market data
2. **Multi-Product Pricing**: Handle product portfolios
3. **Advanced Algorithms**: Experiment with A3C, SAC, TD3
4. **Constraint Handling**: Add business rules (minimum margins)
5. **Deployment**: Create API for real-time recommendations
6. **Multi-Agent**: Simulate competing pricing agents
7. **Explainability**: Add interpretability to pricing decisions

---

## ğŸ“š References

### Papers
1. Schulman, J., et al. (2017). "Proximal Policy Optimization Algorithms"
2. Sutton, R. S., & Barto, A. G. (2018). "Reinforcement Learning: An Introduction"
3. Den Boer, A. V. (2015). "Dynamic Pricing and Learning: Historical Origins, Current Research, and New Directions"

### Documentation
- [Gymnasium (OpenAI Gym)](https://gymnasium.farama.org/)
- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/)
- [PyTorch](https://pytorch.org/)

---

## ğŸ‘¨â€ğŸ“ Academic Use

This project is suitable for:
- âœ… Reinforcement Learning course projects
- âœ… Machine Learning applications
- âœ… Operations Research demonstrations
- âœ… AI/Business optimization case studies

### Learning Outcomes
- Understanding of RL fundamentals (MDP, rewards, policies)
- Practical implementation with Stable-Baselines3
- Custom environment creation with Gymnasium
- Performance evaluation methodologies
- Real-world AI/ML application

---

## ğŸ“ License

This project is created for educational purposes.

---

## ğŸ¤ Contributing

Feel free to:
- Report issues
- Suggest improvements
- Extend the project

---

## ğŸ“§ Contact

For questions or suggestions, please create an issue in the project repository.

---

**ğŸ‰ Happy Learning!**

*Demonstrating the power of Reinforcement Learning in solving real-world business optimization problems.*
